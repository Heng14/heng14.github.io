<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Heng Yu</title>
  
  <meta name="author" content="Heng Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="imgs/heng.png">
  <!-- <script type="text/javascript" src="js/hidebib.js"></script> -->
<!-- <style type="text/css">
</style> -->
<!-- Start : Google Analytics Code -->
<!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-64069893-1', 'auto');
  ga('send', 'pageview');
</script> -->
<!-- End : Google Analytics Code -->
<!-- Scramble Script by Jeff Donahue -->
<!-- <script src="js/scramble.js"></script> -->


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Heng (Henry) Yu</name>
              </p>

              <p align="center">
                <!-- <b>email</b>:&nbsp
                <font id="email" style="display:inline;">
                  <noscript><i>Please enable Javascript to view</i></noscript>
                </font>
                <script>
                emailScramble = new scrambledString(document.getElementById('email'),
                    'emailScramble', 'umepc@aah.tud.skdc',
                    [18,13,16,2,12,8,6,3,5,15,4,14,17,11,10,7,1,9]);
                </script> -->
                hengyu[at]andrew.cmu.edu
              </p>

              <p>I am a master student (MSR) at CMU <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, where I work on 3D vision advised by Prof. <a href="https://www.laszlojeni.com/">Laszlo Jeni </a>. 
                I also closely collaborated with Prof. <a href="https://www.nmr.mgh.harvard.edu/~berkin/index.html">Berkin Bilgic</a> from <a href="https://hms.harvard.edu/"> Harvard Medical School </a> on MRI reconstruction topics and
                 Prof. <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin </a> from 
                 <a href="https://imr.sjtu.edu.cn/en/">Institute of Medical Robotics</a>, Shanghai Jiao Tong University
                  on medical image analysis topics.
              </p>
              <p>
                I obtained my Bachelor's degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University </a> with a major in <a href="https://www.au.tsinghua.edu.cn/">Department of Automation </a>, 
                and a second major in <a href="https://www.sem.tsinghua.edu.cn/">School of Economics and Management </a>.
                During my time at Tsinghua, I worked on computer vision and medical image analysis research under the supervision of 
                Prof. <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou </a>
                and Prof. <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Jianjiang Feng </a> in <a href="http://ivg.au.tsinghua.edu.cn/index.php">Intelligent Vision Group </a>. 
                After that, I co-founded  <a href="https://www.qingtong.cn/">Tsingh Technology </a> focused on intelligent logistics with three PhDs. 
                I used to have a few good times at <a href="http://www.nebula-link.com/">Nebula Link Technology</a> 
                and <a href="https://www.sangfor.com/">Sangfor Technologies Inc</a>.
                
              </p>
              <p>
                I also worked with Prof. <a href="https://kriskitani.github.io/">Kris Kitani</a> from <a href="https://www.ri.cmu.edu/">CMU RI</a>,
                Prof. <a href="https://med.stanford.edu/setsompoplab.html">Kawin Setsompop</a> and 
                Prof. <a href="https://profiles.stanford.edu/ruijiang-li">Ruijiang Li</a> from <a href="https://med.stanford.edu/">Stanford Medicine</a>.
                My research interests lie at the intersection of computer vision and medical image. 
              </p>
              <p style="text-align:center">
                <a href="cv_heng.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=fOCAyuIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Heng14">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/henry-yu-400a74134/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="imgs/heng0.png"><img style="width:100%;max-width:100%" alt="profile photo" src="imgs/heng0.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <!-- <p>In reverse chronological order:</p> -->
            <ul>
              <li>
                    <strong>Dec 2022:</strong> CoNFies paper has been nominated as a best paper candidate!              
              <li>
                    <strong>Nov 2022:</strong> Non-pooling Network paper won Best Paper Award at MICAD 2022!              
              <li>
                    <strong>Nov 2022:</strong> Won gold medal at <a href="https://www.pilcchina.org/home">the 8th China International College Students' 'Internet+' Innovation and Entrepreneurship Competition</a>!             
               <li>
                    <strong>Sep 2022:</strong> CoNFies paper is accepted by FG2023, see you in Hawaii!
               <li>
                    <strong>May 2022:</strong> I am joining Fujitsu this summer as a machine learning intern, working on controllable NeRF.
               <li>
                    <strong>Feb 2022:</strong> One paper is accepted by Magnetic Resonance in Medicine!
              <li>
                    <strong>Aug 2021:</strong> Start my graduate study at CMU RI!                       
              <li>
                    <strong>May 2021:</strong> I am joining Sangfor this summer as a machine learning engineer, working on evading web application firewalls with reinforcement learning.
              <li>
                    <strong>Mar 2021:</strong> One paper is accepted by Nature Communications!    
              <li>
                    <strong>Feb 2021:</strong> eRAKI abstract is accepted by ISMRM2021 as an oral! 
              <li>
                    <strong>Apr 2020:</strong> MixModule paper is accepted by ISBI2020!                     
              <li>
                    <strong>Nov 2019:</strong> One paper is accepted by Annals of Surgery!                 
            <ul>

            </p>
          </td>
        </tr>


      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, MRI reconstruction and medical image analysis. 
                I would like to explore the possibility of applying AI represented by vision technology in the medical field and help improve healthcare and clinical diagnosis.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications</heading>
            <p>
              * refers to co-first author. Please refer to my <a href="https://scholar.google.com/citations?user=fOCAyuIAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>google scholar</a> for more details.
            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/fg.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2211.08610.pdf">
                <papertitle>CoNFies: Controllable Neural Face Avatars</papertitle>
              </a>
              <br>
              <strong>Heng Yu</strong>,  
              <a href="https://scholar.google.com/citations?user=AFaeUrYAAAAJ&hl=en">Koichiro Niinuma</a>              
              <a href="https://www.laszlojeni.com/">László A. Jeni</a>, 
              <br>
              <em>International Conference on Automatic Face and Gesture Recognition (<b><i>FG</i></b>), 2023</em> - <b><i>Best Paper Award Finalist</i></b>
              <br>
              <a href="https://arxiv.org/pdf/2211.08610.pdf">paper</a>
              /
              <a href="https://confies.github.io/">project page</a>
              <p> We propose a fully-automatic controllable neural representation for face self-portraits.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/mrm.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.29036">
                <papertitle>Scan-specific artifact reduction in k-space (SPARK) neural networks synergize with physics-based reconstruction to accelerate MRI</papertitle>
              </a>
              <br>
              <a href="https://yaminarefeen.github.io./">Yamin Arefeen</a>, <a href="https://bekeronur.github.io/">Onur Beker</a>,
              <a href="https://www.nmr.mgh.harvard.edu/user/4554692">Jaejin Cho</a>, <strong>Heng Yu</strong>,  
              <a href="https://www.rle.mit.edu/people/directory/elfar-adalsteinsson/">Elfar Adalsteinsson</a>, <a href="https://www.nmr.mgh.harvard.edu/~berkin/index.html">Berkin Bilgic</a>
              <br>
              <em>Magnetic Resonance in Medicine (<b><i>MRM</i></b>), 2022</em> 
              <br>
              <a href="https://arxiv.org/pdf/2104.01188.pdf">paper</a>
              /
              <a href="https://github.com/YaminArefeen/spark_mrm_2021">code</a>
              <p> We develop a scan-specific model that estimates and corrects k-space errors made when reconstructing accelerated MRI data.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/eraki.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://archive.ismrm.org/2021/0273.html">
                <papertitle>eRAKI: Fast Robust Artificial neural networks for K-space Interpolation (RAKI) with Coil Combination and Joint Reconstruction</papertitle>
              </a>
              <br>
              <strong>Heng Yu</strong>, <a href="https://scholar.google.ch/citations?user=vvgYggEAAAAJ&hl=en">Zijing Dong</a>, 
              <a href="https://yaminarefeen.github.io./">Yamin Arefeen</a>, <a href="https://profiles.stanford.edu/congyu-liao">Congyu Liao</a>,
              <a href="https://med.stanford.edu/setsompoplab.html">Kawin Setsompop</a>, <a href="https://www.nmr.mgh.harvard.edu/~berkin/index.html">Berkin Bilgic</a>
              <br>
              <em>Proceedings of the 29th Annual Meeting of <b><i>ISMRM</i></b>, 2021</em> - <b><i>Oral Presentation</i></b>
              <br>
              <a href="https://martinos.org/~berkin/eraki.pdf">paper</a>
              /
              <a href="https://github.com/Heng14/eRAKI">code</a>
              <p> We accelerate RAKI by more than 200 times by directly learning a coil-combined target.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/nc.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41467-021-22188-y">
                <papertitle>Predicting treatment response from longitudinal images using multi-task deep learning</papertitle>
              </a>
              <br>
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin</a>*, <strong>Heng Yu</strong>*, Jia Ke*, Peirong Ding*,
              Yongju Yi, Xiaofeng Jiang, Xin Duan, Jinghua Tang,  <a href="https://profiles.stanford.edu/daniel-changl">Daniel T. Chang</a>,
              Xiaojian Wu, Feng Gao, <a href="https://profiles.stanford.edu/ruijiang-li">Ruijiang Li</a>
              <br>
              <em><b><i>Nature Communications</i></b>, 2021</em> 
              <br>
              <a href="https://www.nature.com/articles/s41467-021-22188-y">paper</a>
              /
              <a href="https://github.com/Heng14/3D_RP-Net">code</a>
              <p> We present a multi-task deep learning approach that allows simultaneous tumor segmentation and response prediction
                of pathologic complete response after neoadjuvant chemoradiotherapy.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic20_01.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9098498">
                <papertitle>MixModule: Mixed CNN Kernel Module for Medical Image Segmentation</papertitle>
              </a>
              <br>
              <strong>Heng Yu</strong>, Xue Feng, Ziwen Wang, Hao Sun
              <br>
              <em>IEEE 17th International Symposium on Biomedical Imaging (<b><i>ISBI</i></b>), 2020</em>
              <br>
              <a href="https://arxiv.org/abs/1910.08728">paper</a>
              /
              <a href="https://github.com/Heng14/MixModule">code</a>
              <p>We use mixed kernels to improve the performance of existing medical image segmentation networks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic20_02.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://journals.lww.com/annalsofsurgery/Abstract/2021/12000/Development_and_Validation_of_a_Deep_Learning_CT.415.aspx">
                <papertitle>Development and validation of a deep learning CT signature to predict survival and chemotherapy benefit in gastric cancer: a multicenter, retrospective study.</papertitle>
              </a>
              <br>
              <a href="https://profiles.stanford.edu/yuming-jiang">Yuming Jiang</a>*,
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin</a>*, <strong>Heng Yu</strong>*,
              <a href="https://faculty.mdanderson.org/profiles/jia_wu.html">Jia Wu</a>*,
              Chuanli Chen, Qingyu Yuan, Weicai Huang, Yanfeng Hu, Yikai Xu, Zhiwei Zhou,
              <a href="https://profiles.stanford.edu/george-fisher">George A. Fisher Jr.</a>,
              <a href="https://scholar.google.com/citations?user=5cF_c0YAAAAJ&hl=en">Guoxin Li</a>,
              <a href="https://profiles.stanford.edu/ruijiang-li">Ruijiang Li</a>
              <br>
              <em><b><i>Annals of surgery</i></b>, 2020</em> 
              <br>
              <a href="https://journals.lww.com/annalsofsurgery/Abstract/2021/12000/Development_and_Validation_of_a_Deep_Learning_CT.415.aspx">paper</a>
              /
              <a href="https://github.com/Heng14/snet">code</a>
              <p>We propose a novel deep neural network (S-net) to construct a CT signature for predicting disease-free survival (DFS) and overall survival.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic20_03.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://academic.oup.com/bjs/article/108/5/542/6287163">
                <papertitle>Deep learning analysis of the primary tumour and the prediction of lymph node metastases in gastric cancer.</papertitle>
              </a>
              <br>
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">C Jin</a>*,
              <a href="https://profiles.stanford.edu/yuming-jiang">Y Jiang</a>*, <strong>H Yu</strong>*,
              W Wang, B Li, C Chen, Q Yuan, Y Hu, Y Xu, Z Zhou,
              <a href="https://scholar.google.com/citations?user=5cF_c0YAAAAJ&hl=en">G Li</a>,
              <a href="https://profiles.stanford.edu/ruijiang-li">R Li</a>
              <br>
              <em><b><i> British Journal of Surgery</i></b>, 2020</em> 
              <br>
              <a href="https://academic.oup.com/bjs/article/108/5/542/6287163">paper</a>
              /
              <a href="https://github.com/Heng14/multi-LNMACDSS">code</a>
              <p>We develop a deep learning system for predicting lymph node metastasis in multiple nodal stations 
                based on preoperative CT images in patients with gastric cancer.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic18_01.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/8354230">
                <papertitle>SmartPartNet: Part-Informed Person Detection for Body-Worn Smartphones.</papertitle>
              </a>
              <br>
              <strong>H Yu</strong>*,
              <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>,
              <a href="https://scholar.google.com/citations?user=hIEHvCAAAAAJ&hl=en">Donghyun Yoo</a>,
              <a href="https://kriskitani.github.io/">Kris Kitani</a>
              <br>
              <em>IEEE/CVF Winter Conference on Applications of Computer Vision (<b><i>WACV</i></b>), 2018</em> 
              <br>
              <a href="https://ieeexplore.ieee.org/document/8354230">paper</a>
              <p>We develop an image-based person detection algorithm for wearable computing using commodity smartphones.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic18_02.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8260863">
                <papertitle>Left atrial appendage segmentation using fully convolutional neural networks and modified three-dimensional conditional random fields.</papertitle>
              </a>
              <br>
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin</a>,  <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Jianjiang Feng </a>, Lei Wang,
              <strong>Heng Yu</strong>, <a href="https://aiem.jhu.edu/people/jiang-liu/">Jiang Liu</a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou </a>
              <br>
              <em>IEEE Journal of Biomedical and Health Informatics (<b><i>JBHI</i></b>), 2018</em> 
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8260863">paper</a>
              <p>We propose a robust method for automatic left atrial appendage segmentation on computed tomographic angiography data 
                using fully convolutional neural networks with 3D conditional random fields.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic17_01.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-75541-0_5">
                <papertitle>Detection of Substances in the Left Atrial Appendage by Spatiotemporal Motion Analysis Based on 4D-CT.</papertitle>
              </a>
              <br>
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin</a>, <strong>Heng Yu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Jianjiang Feng </a>, Lei Wang,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou </a>
              <br>
              <em><b><i>MICCAI</i></b> workshop, 2017</em> - <b><i>Oral Presentation</i></b>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-75541-0_5">paper</a>
              <p>we present a new approach for the detection of substances in the left atrial appendage by spatiotemporal motion analysis and
                 make a detailed judgment and analysis of spatial distribution and classification of most objects in the left atrial appendage.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="imgs/pic17_02.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-75541-0_4">
                <papertitle>Left atrial appendage neck modeling for closure surgery.</papertitle>
              </a>
              <br>
              <a href="https://imr.sjtu.edu.cn/sz_teachers/3770.html">Cheng Jin</a>, <strong>Heng Yu</strong>, 
              <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Jianjiang Feng </a>, Lei Wang,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou </a>
              <br>
              <em><b><i>MICCAI</i></b> workshop, 2017</em>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-75541-0_4">paper</a>
              <p>We propose a robust method for automatic left atrial appendage segmentation on computed tomographic angiography data 
                using fully convolutional neural networks with 3D conditional random fields.</p>
            </td>
          </tr>

        </tbody></table>

			

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <a href="https://clustrmaps.com/site/1brdz"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=TQ_h6GQFQiPR5Kz3C1hctUGHbF_mdjRJPtjFDl_ATPY&cl=ffffff" /></a>
            <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=TQ_h6GQFQiPR5Kz3C1hctUGHbF_mdjRJPtjFDl_ATPY"></script> -->
          </td>
        </tr>
        </tbody></table>

       

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
